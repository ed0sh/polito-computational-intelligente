{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: Sunday, December 17 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "* Reviews: Dies Natalis Solis Invicti ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T17:39:18.029641Z",
     "start_time": "2023-12-22T17:39:17.439809Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict, namedtuple\n",
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T17:39:18.035480Z",
     "start_time": "2023-12-22T17:39:18.030342Z"
    }
   },
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])\n",
    "class Player(Enum):\n",
    "    X = 1,\n",
    "    O = 0\n",
    "# Reward values\n",
    "WIN_SCORE = 2\n",
    "LOSE_SCORE = -5\n",
    "DRAW_SCORE = -1\n",
    "\n",
    "TRAINING_EPOCHS = 500_000\n",
    "\n",
    "NUM_TEST_GAMES = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Game index map is used to map the game board to another game board, in the second one the numbers are in a spiral order üåÄ\n",
    " 0 1 2 ‚ûú 0 1 2 \n",
    "\n",
    " 3 4 5 ‚ûú 7 8 3  \n",
    "\n",
    " 6 7 8 ‚ûú 6 5 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T17:39:18.035980Z",
     "start_time": "2023-12-22T17:39:18.033004Z"
    }
   },
   "outputs": [],
   "source": [
    "GAME_INDEX_MAP = {\n",
    "    0: 0, 1: 1, 2: 2,\n",
    "    7: 3, 8: 4, 3: 5,\n",
    "    6: 6, 5: 7, 4: 8\n",
    "}\n",
    "\n",
    "# The magic square is used to calculate the score of a game board\n",
    "MAGIC = [4, 9, 2,\n",
    "         3, 5, 7,\n",
    "         8, 1, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T17:39:18.045810Z",
     "start_time": "2023-12-22T17:39:18.040056Z"
    }
   },
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.state = State(set(), set())\n",
    "        self.move = 0\n",
    "        self.my_player = Player.X\n",
    "\n",
    "    def sum_magic(self, comb: tuple) -> int:\n",
    "        return sum(MAGIC[GAME_INDEX_MAP[i]] for i in comb)\n",
    "\n",
    "    def check_win(self, player: Player) -> bool:\n",
    "        '''check if the player win the game'''\n",
    "        if self.move < 5:\n",
    "            return False\n",
    "        if player == Player.X:\n",
    "            return any(self.sum_magic(comb) == 15 for comb in itertools.combinations(self.state.x, 3))\n",
    "        if player == Player.O:\n",
    "            return any(self.sum_magic(comb) == 15 for comb in itertools.combinations(self.state.o, 3))\n",
    "\n",
    "    def check_draw(self) -> bool:\n",
    "        if self.move == 9 and not self.check_win(Player.X) and not self.check_win(Player.O):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def move_done(self, move: int, player: Player) -> bool:\n",
    "        self.move += 1\n",
    "        if player == Player.X:\n",
    "            self.state.x.add(move)\n",
    "        elif player == Player.O:\n",
    "            self.state.o.add(move)\n",
    "        # self.good_print()\n",
    "\n",
    "    def evaluate_match(self) -> int:\n",
    "        if self.check_win(self.my_player):\n",
    "            return 1\n",
    "        elif self.check_win(Player.O if self.my_player == Player.X else Player.X):\n",
    "            return -1\n",
    "        elif self.check_draw():\n",
    "            return 0\n",
    "\n",
    "    def good_print(self):\n",
    "        '''\n",
    "        Print the game board in a nice way, using the spiral order\n",
    "        '''\n",
    "        num = ['0Ô∏è‚É£', '1Ô∏è‚É£', '2Ô∏è‚É£', '7Ô∏è‚É£', '8Ô∏è‚É£', '3Ô∏è‚É£', '6Ô∏è‚É£', '5Ô∏è‚É£', '4Ô∏è‚É£']\n",
    "        counter = 0\n",
    "        for r in range(3):\n",
    "            print('|', end='')\n",
    "            for c in range(3):\n",
    "                val = r * 3 + c\n",
    "                if val in [GAME_INDEX_MAP[i] for i in self.state.x]:\n",
    "                    print('‚úñÔ∏è|', end='')\n",
    "                elif val in [GAME_INDEX_MAP[i] for i in self.state.o]:\n",
    "                    print('‚≠ï|', end='')\n",
    "                else:\n",
    "                    print(f'{num[counter]}|', end=\"\")\n",
    "                counter += 1\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetry function\n",
    "\n",
    "This two symmetry functions are used to check if there is already a similar state in the Q table, if there is one, the Q value of the current state is updated and no new state is added. \n",
    "This approach is used to reduce the size of the Q table and to speed up the learning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T17:39:18.046400Z",
     "start_time": "2023-12-22T17:39:18.043074Z"
    }
   },
   "outputs": [],
   "source": [
    "def rotate_90_right(value):\n",
    "    if value < 6:\n",
    "        new_value = value + 2\n",
    "    elif 6 <= value <= 7:\n",
    "        new_value = value - 6\n",
    "    else:\n",
    "        new_value = value\n",
    "    return new_value\n",
    "\n",
    "\n",
    "def rotate_state_90_right(policy: tuple[tuple[frozenset, frozenset], int]) -> tuple[tuple[frozenset, frozenset], int]:\n",
    "    state, action = policy\n",
    "    rotated_x = set([rotate_90_right(value) for value in state[0]])\n",
    "    rotated_o = set([rotate_90_right(value) for value in state[1]])\n",
    "    rotated_action = rotate_90_right(action)\n",
    "\n",
    "    return (frozenset(rotated_x), frozenset(rotated_o)), rotated_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T17:39:18.060225Z",
     "start_time": "2023-12-22T17:39:18.050583Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReinforcedPlayer:\n",
    "    def __init__(self):\n",
    "        self.Q = defaultdict(float)\n",
    "        self.epsilon = 0.01\n",
    "        self.training_epochs = TRAINING_EPOCHS\n",
    "\n",
    "    def get_Q_value(self, state, action):\n",
    "        st = (frozenset(state.x), frozenset(state.o))\n",
    "\n",
    "        policy = (st, action)\n",
    "        policy_90 = rotate_state_90_right(policy)\n",
    "        policy_180 = rotate_state_90_right(policy_90)\n",
    "        policy_270 = rotate_state_90_right(policy_180)\n",
    "\n",
    "        if policy not in self.Q and policy_90 not in self.Q \\\n",
    "                and policy_180 not in self.Q and policy_270 not in self.Q:\n",
    "            self.Q[policy] = 0.0\n",
    "\n",
    "        if policy_90 in self.Q:\n",
    "            return self.Q[policy_90]\n",
    "        elif policy_180 in self.Q:\n",
    "            return self.Q[policy_180]\n",
    "        elif policy_270 in self.Q:\n",
    "            return self.Q[policy_270]\n",
    "        else:\n",
    "            return self.Q[policy]\n",
    "\n",
    "    def update_Q_value(self, state, action: int, reward):\n",
    "        '''\n",
    "        Update the Q value of the policy (state, action) using the reward.\n",
    "        This function checks the symmetry of the state and update the value of the state adding the reward weighted by the epsilon value.\n",
    "        '''\n",
    "        st = (frozenset(state.x), frozenset(state.o))\n",
    "\n",
    "        policy = (st, action)\n",
    "        policy_90 = rotate_state_90_right(policy)\n",
    "        policy_180 = rotate_state_90_right(policy_90)\n",
    "        policy_270 = rotate_state_90_right(policy_180)\n",
    "\n",
    "        if policy_90 in self.Q:\n",
    "            self.Q[policy_90] = self.get_Q_value(state, action) + self.epsilon * (reward - self.get_Q_value(state, action))\n",
    "        elif policy_180 in self.Q:\n",
    "            self.Q[policy_180] = self.get_Q_value(state, action) + self.epsilon * (reward - self.get_Q_value(state, action))\n",
    "        elif policy_270 in self.Q:\n",
    "            self.Q[policy_270] = self.get_Q_value(state, action) + self.epsilon * (reward - self.get_Q_value(state, action))\n",
    "        else:\n",
    "            self.Q[policy] = self.get_Q_value(state, action) + self.epsilon * (reward - self.get_Q_value(state, action))\n",
    "\n",
    "    def choose_action(self, state, available_moves):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(available_moves)\n",
    "        else:\n",
    "            Q_values = [self.get_Q_value(state, action) for action in available_moves]\n",
    "            max_Q = max(Q_values)\n",
    "            if Q_values.count(max_Q) > 1:\n",
    "                best_moves = [i for i in range(len(available_moves)) if Q_values[i] == max_Q]\n",
    "                i = random.choice(best_moves)\n",
    "            else:\n",
    "                i = Q_values.index(max_Q)\n",
    "            return available_moves[i]\n",
    "\n",
    "    def training(self):\n",
    "        for _ in tqdm(range(self.training_epochs)):\n",
    "            new_game = TicTacToe()\n",
    "            trajectory, reward = self.random_game(new_game)\n",
    "            ## update the Q value for each tuple (state, action) in the trajectory\n",
    "            for state_move in trajectory:\n",
    "                self.update_Q_value(state=state_move[0], action=state_move[1], reward=reward)\n",
    "        save_model(self,\n",
    "                   f\"EPOCHS_{str(self.training_epochs)}-LOSE_{LOSE_SCORE}-WIN_{WIN_SCORE}-DRAW_{DRAW_SCORE}-NUOVA_RAPP\")\n",
    "    \n",
    "    def random_game(self, game: TicTacToe):\n",
    "        '''\n",
    "        play a semi random game and return the trajectory of the chosen player (X or O) and the reward.\n",
    "        The reward is 1 if the player win, -1 if the player lose and 0 if it is a draw\n",
    "        '''\n",
    "        trajectory = list()\n",
    "        available_moves = list(range(0, 9))\n",
    "        ## a random player start\n",
    "        turn = np.random.choice([0, 1])\n",
    "\n",
    "        players = [Player.X, Player.O]\n",
    "        game.my_player = random.choice(players)  ## train the model on random player\n",
    "        # so it is possible to play with both X and O\n",
    "        while len(available_moves) != 0 and not game.check_draw():\n",
    "            turn = 1 - turn\n",
    "\n",
    "            if game.my_player == players[turn]:\n",
    "                move = self.choose_action(game.state, available_moves)\n",
    "                trajectory.append((deepcopy(game.state), move))\n",
    "            else:\n",
    "                move = np.random.choice(available_moves)\n",
    "\n",
    "            available_moves.remove(move)\n",
    "            game.move_done(move, players[turn])\n",
    "\n",
    "            if game.check_win(players[turn]):\n",
    "                if game.my_player == players[turn]:\n",
    "                    return trajectory, WIN_SCORE\n",
    "                else:\n",
    "                    return trajectory, LOSE_SCORE\n",
    "\n",
    "        return trajectory, DRAW_SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T17:39:18.060784Z",
     "start_time": "2023-12-22T17:39:18.053451Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to save the class trained player, to avoid to train the model every time\n",
    "\n",
    "def save_model(model: ReinforcedPlayer, text: str = None):\n",
    "    # Serialize the object and write it to a file\n",
    "    with open(f'models/agent-{text}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_model(path: str) -> ReinforcedPlayer:\n",
    "    # Load the model from a file\n",
    "    with open(path, 'rb') as f:\n",
    "        loaded_instance = pickle.load(f)\n",
    "\n",
    "    return loaded_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T17:40:51.950226Z",
     "start_time": "2023-12-22T17:39:18.055739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81b5164f3c3b498da2365f9c60044e9a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = True\n",
    "\n",
    "if train:\n",
    "    rr = ReinforcedPlayer()\n",
    "    rr.training()\n",
    "    #rr2 = load_model('models/agent-EPOCHS_10000000.pkl')\n",
    "else:\n",
    "    pass\n",
    "    #rr = load_model('models/agent-EPOCHS_1000000-LOSE_-5-WIN_2-DRAW_-1-NUOVA_RAPP.pkl')\n",
    "    #rr2 = load_model('models/agent-EPOCHS_100000-LOSE_-5-WIN_2-DRAW_-1-NUOVA_RAPP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T17:49:04.999705Z",
     "start_time": "2023-12-22T17:49:04.137044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins : 9196, draws: 708, lost: 96\n"
     ]
    }
   ],
   "source": [
    "wins = 0\n",
    "draws = 0\n",
    "for _ in range(NUM_TEST_GAMES):\n",
    "    game = TicTacToe()\n",
    "    turn = np.random.choice([0, 1])\n",
    "    players = [Player.X, Player.O]\n",
    "    rr_player = random.choice(players)\n",
    "    available_moves = list(range(0, 9))\n",
    "    while len(available_moves) != 0 and not game.check_draw():\n",
    "        turn = 1 - turn\n",
    "        # game.good_print()\n",
    "        if players[turn] == rr_player:\n",
    "            move = rr.choose_action(game.state, available_moves)\n",
    "        else:\n",
    "            move = random.choice(available_moves)\n",
    "            # move = int(input(\"Enter your move: \"))\n",
    "            # move = rr2.choose_action(game.state, available_moves)\n",
    "\n",
    "        available_moves.remove(move)\n",
    "        game.move_done(move, players[turn])\n",
    "        if game.check_win(players[turn]):\n",
    "            # game.good_print()\n",
    "            if players[turn] == rr_player:\n",
    "                wins += 1\n",
    "            break\n",
    "\n",
    "    if game.check_draw():\n",
    "        draws += 1\n",
    "\n",
    "print(f'wins : {wins}, draws: {draws}, lost: {NUM_TEST_GAMES - wins - draws}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_CI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
